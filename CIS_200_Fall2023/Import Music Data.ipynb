{"cells":[{"cell_type":"markdown","source":["# Import Music Data\n","\n","Sources: \n","- [Multiple CSV files to build a relational database](https://www.kaggle.com/datasets/mcfurland/10-m-beatport-tracks-spotify-audio-features/data)\n","- [Single CSV file (music.csv) for simplified analysis](https://github.com/likeawednesday/TechCamp_DataViz/blob/main/data/music.csv)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"efd9fbd8-f992-44a9-9e22-3b2ac561396a"},{"cell_type":"code","source":["import os\n","import zipfile\n","from pyspark.sql.functions import col, when\n","from pyspark.sql.types import IntegerType, StringType, BooleanType, DateType\n","\n","# Specify zip file name\n","zip_file = 'multi_table.zip'\n","\n","# Specify schemas to import\n","import_schemas = (\n","\n","    # beatport\n","    'bp'\n","\n","    # spotify\n","    , 'sp'\n",")\n","\n","# Specify base path\n","base_path = os.path.join('/', 'lakehouse', 'default')\n","\n","# Specify spark path\n","spark_path = 'Files'\n","\n","# Get folder name from zip file\n","dataset_folder = zip_file.rstrip('.zip')\n","\n","# Specify folder path\n","folder_path = os.path.join(base_path, spark_path, dataset_folder)\n","\n","# Specify zip file path\n","zip_file_path = os.path.join(base_path, spark_path, dataset_folder + '.zip')\n","\n","# Get list of schemas from folder_path\n","schemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4f9df2dd-6a37-4cbf-9338-00d77746a321"},{"cell_type":"code","source":["# Extract zip file\n","with zipfile.ZipFile(zip_file_path, 'r') as archive:\n","  \n","  # Loop through all files in archive\n","  for file in archive.namelist():\n","\n","    # Get schema from characters before the first underscore\n","    schema = file.split('_', 1)[0]\n","\n","    # Set destination for extracted files\n","    destination = os.path.join(folder_path, schema)\n","\n","    # Only extract files matching import_schemas\n","    if file.startswith(import_schemas):\n","\n","      # Create a new directory for the destination if it doesn't exist\n","      if not os.path.exists(destination):\n","        os.makedirs(destination)\n","\n","      # Extract file into the destination directory\n","      print(f\"Extracting: {file}\")\n","      archive.extract(file, destination)\n","\n","archive.close()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"073c546a-5a62-45f1-8373-6e2beeee914f"},{"cell_type":"code","source":["# Loop through schemas and import their tables\n","for schema in schemas:\n","\n","    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n","    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n","    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n","\n","    # Construct schema_path from folder_path and schema\n","    schema_path = os.path.join(folder_path, schema)\n","\n","    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n","    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n","\n","    # Loop through each CSV file\n","    for file in files:\n","        \n","        # Get table name from CSV file, adding extra underscore to schema prefix\n","        prefix = schema + '_'\n","        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n","\n","        # Construct spark_schema_path\n","        spark_schema_path = os.path.join(spark_path, dataset_folder, schema, file)\n","        \n","        # Read CSV file into DataFrame\n","        df = spark.read.format('csv') \\\n","            .option('header', 'true') \\\n","            .option('mode', 'DROPMALFORMED') \\\n","            .option('inferschema', 'true') \\\n","            .option('dateFormat', 'yyyy-mm-dd') \\\n","            .load(spark_schema_path)\n","\n","        # Use column names as clues to set their datatypes\n","        for column in df.columns:\n","\n","            # ID columns\n","            id_names = ('_id')\n","            if column.endswith(id_names) and schema == 'bp':\n","                df = df.withColumn(column, col(column).cast(IntegerType()))\n","\n","            # Integer columns\n","            integer_names = ('_count', '_num', '_number', '_ms', 'popularity', '_tracks', 'bpm')\n","            if column.endswith(integer_names):\n","                df = df.withColumn(column, col(column).cast(IntegerType()))\n","            \n","            # Boolean columns\n","            boolean_names = ('explicit', 'is_')\n","            if column.startswith(boolean_names):\n","                df = df.withColumn(column, when(col(column) == 't', True).otherwise(False))\n","                df = df.withColumn(column, col(column).cast(BooleanType()))\n","\n","            # String columns\n","            string_names = ('_name', '_letter', '_url', '_uuid')\n","            if column.endswith(string_names):\n","                df = df.withColumn(column, col(column).cast(StringType()))\n","\n","            # Date columns\n","            date_names = ('_on', '_date')\n","            if column.endswith(date_names):\n","                df = df.withColumn(column, col(column).cast(DateType()))\n","\n","        # Write DataFrame into Delta table\n","        print(f\"Importing table: {table_name}\")\n","        df.write.format('delta') \\\n","            .mode('overwrite') \\\n","            .saveAsTable(table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"50601954-7cb2-438f-97a4-70bc9d4fcab2"},{"cell_type":"code","source":["# To delete all CSV files, drop all tables, and start over...\n","# Un-freeze and run this cell\n","\n","# Delete the folder containing the CSV files\n","# print(f\"Deleting folder: {folder_path}\")\n","os.rmdir(folder_path)\n","\n","# Get name of current database\n","database_name = spark.catalog.currentDatabase()\n","\n","# Get a list of all tables\n","tables = spark.catalog.listTables()\n","\n","# Drop all delta tables\n","for table in tables:\n","    print(f\"Dropping table: {table.name}\")\n","    spark.sql(f\"DROP TABLE IF EXISTS {database_name}.{table.name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":false,"run_control":{"frozen":true}},"id":"a91a2a08-6560-426c-998c-ca9649f950e4"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"31c73961-259e-48db-bfc2-920962f05912","known_lakehouses":[{"id":"31c73961-259e-48db-bfc2-920962f05912"}],"default_lakehouse_name":"CIS_200_MusicData","default_lakehouse_workspace_id":"864f8700-782b-4be7-8843-9f8ac9c049f3"}}},"nbformat":4,"nbformat_minor":5}