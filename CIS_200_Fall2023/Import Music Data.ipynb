{"cells":[{"cell_type":"markdown","source":["# Import Music Data\n","\n","Sources: \n","- [Multiple CSV files to build a relational database](https://www.kaggle.com/datasets/mcfurland/10-m-beatport-tracks-spotify-audio-features/data)\n","- [Single CSV file (music.csv) for simplified analysis](https://github.com/likeawednesday/TechCamp_DataViz/blob/main/data/music.csv)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"efd9fbd8-f992-44a9-9e22-3b2ac561396a"},{"cell_type":"code","source":["import os\n","import zipfile\n","from pyspark.sql.functions import col, when\n","from pyspark.sql.types import IntegerType, StringType, BooleanType, DateType\n","\n","# Specify zip file name\n","zip_file = 'multi_table.zip'\n","\n","# Specify schemas to extract and import\n","import_schemas = (\n","\n","    # beatport\n","    'bp'\n","\n","    # spotify\n","    , 'sp'\n",")\n","\n","# Specify base path\n","base_path = os.path.join('/', 'lakehouse', 'default')\n","\n","# Specify spark path\n","spark_path = 'Files'\n","\n","# Get folder name from zip file\n","dataset_folder = zip_file.rstrip('.zip')\n","\n","# Specify folder path\n","folder_path = os.path.join(base_path, spark_path, dataset_folder)\n","\n","# Specify zip file path\n","zip_file_path = os.path.join(base_path, spark_path, dataset_folder + '.zip')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d889a5cb-f9a0-482b-aef2-31119c0c2793","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-11T15:29:25.5082267Z","session_start_time":null,"execution_start_time":"2023-10-11T15:29:25.8112791Z","execution_finish_time":"2023-10-11T15:29:26.1192151Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"5e66e168-d5fb-4f2c-9cef-c7adc36f2ebc"},"text/plain":"StatementMeta(, d889a5cb-f9a0-482b-aef2-31119c0c2793, 8, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4f9df2dd-6a37-4cbf-9338-00d77746a321"},{"cell_type":"code","source":["# Extract zip file\n","with zipfile.ZipFile(zip_file_path, 'r') as archive:\n","  \n","  # Loop through all files in archive\n","  for file in archive.namelist():\n","\n","    # Get schema from characters before the first underscore\n","    schema = file.split('_', 1)[0]\n","\n","    # Set destination for extracted files\n","    destination = os.path.join(folder_path, schema)\n","\n","    # Only extract files matching import_schemas\n","    if file.startswith(import_schemas):\n","\n","      # Create a new directory for the destination if it doesn't exist\n","      if not os.path.exists(destination):\n","        os.makedirs(destination)\n","\n","      # Extract file into the destination directory\n","      print(f\"Extracting: {spark_path}/{dataset_folder}/{schema}/{file}\")\n","      archive.extract(file, destination)\n","\n","archive.close()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d889a5cb-f9a0-482b-aef2-31119c0c2793","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-11T15:38:08.0719195Z","session_start_time":null,"execution_start_time":"2023-10-11T15:38:08.4191409Z","execution_finish_time":"2023-10-11T15:39:18.8261047Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"991f7e30-395e-414d-8446-200ddddd12af"},"text/plain":"StatementMeta(, d889a5cb-f9a0-482b-aef2-31119c0c2793, 14, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting: Files/multi_table/bp/bp_artist.csv\nExtracting: Files/multi_table/bp/bp_artist_media.csv\nExtracting: Files/multi_table/bp/bp_artist_release.csv\nExtracting: Files/multi_table/bp/bp_artist_track.csv\nExtracting: Files/multi_table/bp/bp_genre.csv\nExtracting: Files/multi_table/bp/bp_key.csv\nExtracting: Files/multi_table/bp/bp_label.csv\nExtracting: Files/multi_table/bp/bp_label_artist.csv\nExtracting: Files/multi_table/bp/bp_label_media.csv\nExtracting: Files/multi_table/bp/bp_release.csv\nExtracting: Files/multi_table/bp/bp_release_media.csv\nExtracting: Files/multi_table/bp/bp_subgenre.csv\nExtracting: Files/multi_table/bp/bp_track.csv\nExtracting: Files/multi_table/bp/bp_track_media.csv\nExtracting: Files/multi_table/sp/sp_artist.csv\nExtracting: Files/multi_table/sp/sp_artist_release.csv\nExtracting: Files/multi_table/sp/sp_artist_track.csv\nExtracting: Files/multi_table/sp/sp_release.csv\nExtracting: Files/multi_table/sp/sp_track.csv\n"]}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"073c546a-5a62-45f1-8373-6e2beeee914f"},{"cell_type":"code","source":["# Get list of schemas extracted from zip file\n","schemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n","\n","# Loop through schemas and import their tables\n","for schema in schemas:\n","\n","    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n","    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n","    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n","\n","    # Construct schema_path from folder_path and schema\n","    schema_path = os.path.join(folder_path, schema)\n","\n","    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n","    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n","\n","    # Loop through each CSV file\n","    for file in files:\n","        \n","        # Get table name from CSV file, adding extra underscore to schema prefix\n","        prefix = schema + '_'\n","        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n","\n","        # Construct spark_schema_path\n","        spark_schema_path = os.path.join(spark_path, dataset_folder, schema, file)\n","        \n","        # Read CSV file into DataFrame\n","        df = spark.read.format('csv') \\\n","            .option('header', 'true') \\\n","            .option('mode', 'DROPMALFORMED') \\\n","            .option('inferschema', 'true') \\\n","            .option('dateFormat', 'yyyy-mm-dd') \\\n","            .load(spark_schema_path)\n","\n","        # Use column names as clues to set their datatypes\n","        for column in df.columns:\n","\n","            # ID columns\n","            id_names = ('_id')\n","            if column.endswith(id_names) and schema == 'bp':\n","                df = df.withColumn(column, col(column).cast(IntegerType()))\n","\n","            # Integer columns\n","            integer_names = ('_count', '_num', '_number', '_ms', 'popularity', '_tracks', 'bpm')\n","            if column.endswith(integer_names):\n","                df = df.withColumn(column, col(column).cast(IntegerType()))\n","            \n","            # Boolean columns\n","            boolean_names = ('explicit', 'is_')\n","            if column.startswith(boolean_names):\n","                df = df.withColumn(column, when(col(column) == 't', True).otherwise(False))\n","                df = df.withColumn(column, col(column).cast(BooleanType()))\n","\n","            # String columns\n","            string_names = ('_name', '_letter', '_url', '_uuid')\n","            if column.endswith(string_names):\n","                df = df.withColumn(column, col(column).cast(StringType()))\n","\n","            # Date columns\n","            date_names = ('_on', '_date')\n","            if column.endswith(date_names):\n","                df = df.withColumn(column, col(column).cast(DateType()))\n","\n","        # Write DataFrame into Delta table\n","        print(f\"Importing table: {table_name}\")\n","        df.write.format('delta') \\\n","            .mode('overwrite') \\\n","            .saveAsTable(table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"d889a5cb-f9a0-482b-aef2-31119c0c2793","statement_id":22,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-11T15:46:06.3192268Z","session_start_time":null,"execution_start_time":"2023-10-11T15:46:06.6348051Z","execution_finish_time":"2023-10-11T15:51:05.664204Z","spark_jobs":{"numbers":{"UNKNOWN":0,"FAILED":0,"RUNNING":0,"SUCCEEDED":209},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4451,"rowCount":50,"usageDescription":"","jobId":216,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 1","submissionTime":"2023-10-11T15:51:02.837GMT","completionTime":"2023-10-11T15:51:02.860GMT","stageIds":[371,372,370],"jobGroup":"22","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4451,"dataRead":3855,"rowCount":57,"usageDescription":"","jobId":215,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 1","submissionTime":"2023-10-11T15:51:02.458GMT","completionTime":"2023-10-11T15:51:02.820GMT","stageIds":[368,369],"jobGroup":"22","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3855,"dataRead":4274,"rowCount":14,"usageDescription":"","jobId":214,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 1","submissionTime":"2023-10-11T15:51:02.304GMT","completionTime":"2023-10-11T15:51:02.359GMT","stageIds":[367],"jobGroup":"22","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":2149,"rowCount":3,"usageDescription":"","jobId":213,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:51:01.813GMT","completionTime":"2023-10-11T15:51:01.899GMT","stageIds":[365,366],"jobGroup":"22","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":569902133,"dataRead":697983951,"rowCount":11555106,"usageDescription":"","jobId":212,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:45.544GMT","completionTime":"2023-10-11T15:51:01.714GMT","stageIds":[363,364],"jobGroup":"22","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":10,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":697983951,"dataRead":0,"rowCount":11555106,"usageDescription":"","jobId":211,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:36.155GMT","completionTime":"2023-10-11T15:50:45.491GMT","stageIds":[362],"jobGroup":"22","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":10,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":10,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4446,"rowCount":50,"usageDescription":"","jobId":210,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 0","submissionTime":"2023-10-11T15:50:35.836GMT","completionTime":"2023-10-11T15:50:35.859GMT","stageIds":[361,359,360],"jobGroup":"22","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4446,"dataRead":1961,"rowCount":54,"usageDescription":"","jobId":209,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 0","submissionTime":"2023-10-11T15:50:35.512GMT","completionTime":"2023-10-11T15:50:35.819GMT","stageIds":[357,358],"jobGroup":"22","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1961,"dataRead":2560,"rowCount":8,"usageDescription":"","jobId":208,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 0","submissionTime":"2023-10-11T15:50:35.353GMT","completionTime":"2023-10-11T15:50:35.403GMT","stageIds":[356],"jobGroup":"22","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at <unknown>:0","dataWritten":0,"dataRead":1278738071,"rowCount":5777708,"usageDescription":"","jobId":207,"name":"load at <unknown>:0","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:29.296GMT","completionTime":"2023-10-11T15:50:34.674GMT","stageIds":[355],"jobGroup":"22","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":10,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":10,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"load at <unknown>:0","dataWritten":0,"dataRead":65536,"rowCount":1,"usageDescription":"","jobId":206,"name":"load at <unknown>:0","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:29.167GMT","completionTime":"2023-10-11T15:50:29.266GMT","stageIds":[354],"jobGroup":"22","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4443,"rowCount":50,"usageDescription":"","jobId":205,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 1","submissionTime":"2023-10-11T15:50:28.399GMT","completionTime":"2023-10-11T15:50:28.425GMT","stageIds":[353,351,352],"jobGroup":"22","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4443,"dataRead":3899,"rowCount":57,"usageDescription":"","jobId":204,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 1","submissionTime":"2023-10-11T15:50:28.042GMT","completionTime":"2023-10-11T15:50:28.359GMT","stageIds":[350,349],"jobGroup":"22","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3899,"dataRead":4354,"rowCount":14,"usageDescription":"","jobId":203,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 1","submissionTime":"2023-10-11T15:50:27.866GMT","completionTime":"2023-10-11T15:50:27.912GMT","stageIds":[348],"jobGroup":"22","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":2159,"rowCount":3,"usageDescription":"","jobId":202,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:27.404GMT","completionTime":"2023-10-11T15:50:27.486GMT","stageIds":[347,346],"jobGroup":"22","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":57903929,"dataRead":80352358,"rowCount":1427056,"usageDescription":"","jobId":201,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:24.820GMT","completionTime":"2023-10-11T15:50:27.287GMT","stageIds":[344,345],"jobGroup":"22","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":80352358,"dataRead":0,"rowCount":1427056,"usageDescription":"","jobId":200,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...","submissionTime":"2023-10-11T15:50:23.473GMT","completionTime":"2023-10-11T15:50:24.781GMT","stageIds":[343],"jobGroup":"22","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4438,"rowCount":50,"usageDescription":"","jobId":199,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 0","submissionTime":"2023-10-11T15:50:23.158GMT","completionTime":"2023-10-11T15:50:23.181GMT","stageIds":[342,340,341],"jobGroup":"22","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4438,"dataRead":1977,"rowCount":54,"usageDescription":"","jobId":198,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 0","submissionTime":"2023-10-11T15:50:22.804GMT","completionTime":"2023-10-11T15:50:23.144GMT","stageIds":[338,339],"jobGroup":"22","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1977,"dataRead":2601,"rowCount":8,"usageDescription":"","jobId":197,"name":"toString at String.java:2994","description":"Delta: Job group for statement 22:\n# Get list of schemas extracted from zip file\nschemas = [s for s in os.listdir(folder_path) if s.startswith(import_schemas)]\n\n# Loop through schemas and import their tables\nfor schema in schemas:\n\n    # Set datetime mode by schema to avoid import issues with very old tracks from spotify\n    datetime_mode = 'LEGACY' if schema == 'sp' else 'CORRECTED'\n    spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', datetime_mode)\n\n    # Construct schema_path from folder_path and schema\n    schema_path = os.path.join(folder_path, schema)\n\n    # Get list of CSV files starting with 'bp_' (beatport) in folder_path\n    files = [f for f in os.listdir(schema_path) if f.endswith('.csv')]\n\n    # Loop through each CSV file\n    for file in files:\n        \n        # Get table name from CSV file, adding extra underscore to schema prefix\n        prefix = schema + '_'\n        table_name = file.rstrip('.csv').replace(prefix, prefix + '_')\n\n        # Construct spark_schema_path\n        spark_schema_path = os.path.join(spark_pa...: Compute snapshot for version: 0","submissionTime":"2023-10-11T15:50:22.646GMT","completionTime":"2023-10-11T15:50:22.696GMT","stageIds":[337],"jobGroup":"22","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"5e2687dd-77f8-4a75-b566-7201379c0171"},"text/plain":"StatementMeta(, d889a5cb-f9a0-482b-aef2-31119c0c2793, 22, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Importing table: bp__artist\nImporting table: bp__artist_media\nImporting table: bp__artist_release\nImporting table: bp__artist_track\nImporting table: bp__genre\nImporting table: bp__key\nImporting table: bp__label\nImporting table: bp__label_artist\nImporting table: bp__label_media\nImporting table: bp__release\nImporting table: bp__release_media\nImporting table: bp__subgenre\nImporting table: bp__track\nImporting table: bp__track_media\nImporting table: sp__artist\nImporting table: sp__artist_release\nImporting table: sp__artist_track\nImporting table: sp__release\nImporting table: sp__track\n"]}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"fc345f0f-2dbb-4fc1-b055-f2ea753a6c08\",\"activityId\":\"d889a5cb-f9a0-482b-aef2-31119c0c2793\",\"applicationId\":\"application_1697037666210_0001\",\"jobGroupId\":\"22\",\"advices\":{\"info\":5}}"}},"id":"50601954-7cb2-438f-97a4-70bc9d4fcab2"},{"cell_type":"code","source":["# WARNING: DO NOT RUN THIS CELL UNLESS YOU REALLY KNOW WHAT YOU ARE DOING\n","# To delete all CSV files, drop all tables, and start over: Un-freeze and run this cell\n","\n","# Delete the folder containing the CSV files\n","print(f\"Deleting folder: {folder_path}\")\n","os.rmdir(folder_path)\n","\n","# Get name of current database\n","database_name = spark.catalog.currentDatabase()\n","\n","# Get a list of all tables\n","tables = spark.catalog.listTables()\n","\n","import time\n","print('⚠ WARNING: 30 SECONDS UNTIL ALL FILES AND TABLES WILL BE DELETED! ⚠')\n","print('⚠ If you did not mean to do this, cancel this cell or stop the session NOW. ⚠')\n","\n","for i in range (0, 30):\n","    print(str(30 - i) + '…')\n","    time.sleep(1)\n","\n","# Drop all delta tables\n","for table in tables:\n","    print(f\"Dropping table: {table.name}\")\n","    spark.sql(f\"DROP TABLE IF EXISTS {database_name}.{table.name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":false,"run_control":{"frozen":true}},"id":"a91a2a08-6560-426c-998c-ca9649f950e4"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"31c73961-259e-48db-bfc2-920962f05912","known_lakehouses":[{"id":"31c73961-259e-48db-bfc2-920962f05912"}],"default_lakehouse_name":"CIS_200_MusicData","default_lakehouse_workspace_id":"864f8700-782b-4be7-8843-9f8ac9c049f3"}}},"nbformat":4,"nbformat_minor":5}